# Experiment Args
exp_name: "rlyx-gsm8k-qwen-2.5-0.5b-instruct-example"

logging_methods: ["wandb"]
# logging_methods: ["tensorboard"]
wandb_project: "rlyx-gsm8k-example"
wandb_entity: "naver-clova"

# Dataset Args
dataset_loader_name: "huggingface_loader"  # Dataset loader module
dataset_name_or_path: "openai/gsm8k"

tokenized_dataset_path: "./data/rlyx-gsm8k-qwen-2.5-0.5b-instruct-example"
overwrite_preprocess: true
train_size_limit: -1  # -1 means use all data
valid_size_limit: 50  # -1 means use all data

# Preprocessing Args
batch_size_for_preproc: 3000
num_proc_for_preproc: 8

# Model Args
model_name_or_path: "Qwen/Qwen2.5-0.5B-Instruct"

# Experiment Module Args
chat_template_name: "default_chat_template"
tokenizer_function_name: "gsm8k_default_chatml_tokenizer"
evaluator_name: "gsm8k_basic_evaluator"

# Training Args
max_length: 500
num_train_epochs: 30
num_warmup_steps: 100
learning_rate: 5.e-7
lr_scheduler_type: "cosine"
max_grad_value: 1.0
train_batch_size_per_proc: 1
eval_batch_size_per_proc: 8
gradient_accumulation_steps: 1

# GRPO Args
reward_function_names: ["default_format_reward", "math_reward"]
rollout_per_sample: 3
rollout_temperature: 1.0
rollout_max_tokens: 500
kl_coef: 0.0
stop_tokens: ["<|im_end|>"]


# Logging and Saving Args
eval_interval: 160 # 60
log_interval: 1
save_interval: 160
save_dir: './ckpts/rlyx-gsm8k-qwen-2.5-0.5b-instruct-example'
